torch==2.4.1
transformers==4.38.2
faiss-gpu==1.7.2
sentence-transformers==4.1.0
requests==2.32.3
numpy==1.26.4
<<<<<<< HEAD

#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import json
import torch
import os
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer
from transformers import AutoModelForSequenceClassification, AutoTokenizer
from config.labels import id2label, reminder_keywords

# Paths
BASE_PATH = "/mnt/c/users/bozok/onedrive/desktop/kalliope/Chatgpt_testground"
INTENT_MODEL_PATH = os.path.join(BASE_PATH, "models", "intent-medium-model-multilang-v2.0")
FAISS_INDEX_PATH = os.path.join(BASE_PATH, "faiss/esdalab_index.bin")
FAISS_TEXTS_PATH = os.path.join(BASE_PATH, "faiss/esdalab_texts.json")
DATASET_PATH = os.path.join(BASE_PATH, "TalkChatBot/dataset/train_dataset/esdalab_english.jsonl")

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def load_intent_model():
    tokenizer = AutoTokenizer.from_pretrained(INTENT_MODEL_PATH)
    model = AutoModelForSequenceClassification.from_pretrained(INTENT_MODEL_PATH).to(DEVICE)
    return tokenizer, model

def load_faiss():
    index = faiss.read_index(FAISS_INDEX_PATH)
    with open(FAISS_TEXTS_PATH, "r") as f:
        texts = json.load(f)
    embedder = SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2")
    return index, texts, embedder

def load_dataset_entries():
    entries = []
    with open(DATASET_PATH, "r") as f:
        for line in f:
            line = line.strip()
            if line:
                try:
                    entries.append(json.loads(line))
                except json.JSONDecodeError:
                    continue
    return entries

def generate_response(query, index, texts, embedder, dataset_entries):
    query_vec = embedder.encode([query])
    D, I = index.search(np.array(query_vec), 1)
    closest_q = texts[I[0][0]]
    closest_answer = next(
        (entry["output"] for entry in dataset_entries if entry["input"].lower() == closest_q.lower()),
        f"I found something similar: {closest_q}"
    )
    return closest_answer

def main():
    # Učitaj modele i podatke
    tokenizer, intent_model = load_intent_model()
    index, texts, embedder = load_faiss()
    dataset_entries = load_dataset_entries()

    print("Enter text to classify (type 'exit' to quit):")
    while True:
        text = input("> ").strip()
        if text.lower() == "exit":
            break
        if not text:
            continue

        # Classify intent
        input_text = f"Classify the intent of the following sentence. {text}"
        inputs = tokenizer(input_text, return_tensors="pt", truncation=True, padding="max_length", max_length=128).to(DEVICE)
        with torch.no_grad():
            outputs = intent_model(**inputs)
        probs = torch.softmax(outputs.logits, dim=1)
        class_id = torch.argmax(probs, dim=1).item()
        predicted_label = id2label[class_id]
        confidence = probs.max().item()

        # Handle reminders
        if any(kw in text for kw in reminder_keywords) and predicted_label not in ["remember-synapse", "remember-todo"]:
            predicted_label = "remember-synapse"

        if confidence >= 0.9 and predicted_label != "generative":
            # Visok confidence -> samo štampaj order
            print(predicted_label)
        else:
            # Fallback: FAISS odgovor
            response = generate_response(text, index, texts, embedder, dataset_entries)
            print("say-dynamic-message")
            print(f"text: {response}")

if __name__ == "__main__":
    main()
=======
>>>>>>> 3b5f54abcbf70d6870f36c270c63f7f7663e3a96
